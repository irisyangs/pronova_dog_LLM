{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pronova LLM Run Model #\n",
    "## Use this notebook to do the following ##\n",
    "- Run the current model on a query\n",
    "- start a flask api server that accepts a query and will return a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irisyang1010/Documents/Capstone/pronova_dog_LLM/.venv/lib/python3.8/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load require libraries\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Qdrant connection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Qdrant\n"
     ]
    }
   ],
   "source": [
    "# Get the Qdrant API key from the environment variable\n",
    "Qdrant_api_key = os.getenv('Qdrant_API_KEY')\n",
    "if not Qdrant_api_key:\n",
    "    raise ValueError(\"No Qdrant API key found in environment variables\")\n",
    "Qdrant_url = os.getenv('Qdrant_URL')\n",
    "if not Qdrant_url:\n",
    "    raise ValueError(\"No Qdrant URL found in environment variables\")\n",
    "\n",
    "\n",
    "# Initialize Qdrant client\n",
    "try:\n",
    "    Qclient = QdrantClient(\n",
    "        url= Qdrant_url,\n",
    "        api_key=Qdrant_api_key\n",
    "    )\n",
    "    print(\"Successfully connected to Qdrant\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Qdrant: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup OpenAI connection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OpenAI API key from the environment variable\n",
    "OpenAI_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not OpenAI_api_key:\n",
    "    raise ValueError(\"No OpenAI API key found in environment variables\")\n",
    "\n",
    "OpenAI.api_key = OpenAI_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an OpenAI embedding from a text segment (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the embedding of a text\n",
    "def get_embedding(text):\n",
    "    client = OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve similar chunks from query (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(collection_name, query, top_k=10):\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    search_result = Qclient.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    contexts = [result.payload[\"text\"] for result in search_result]\n",
    "    files = [result.payload.get(\"source_file\") for result in search_result]\n",
    "    \n",
    "    return contexts, files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank response source importance (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def file_ratios(files):\n",
    "    total_files = len(files)\n",
    "    counts = Counter(files)\n",
    "    return {file: count*100 / total_files for file, count in counts.items()}\n",
    "\n",
    "\n",
    "# file_ratios([\"a\", \"a\", \"b\", \"c\"])\n",
    "# {'a': 0.5, 'b': 0.25, 'c': 0.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Print Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_markdown(md_text):\n",
    "    display(Markdown(md_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Response from Query (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_response(collection_name, query, all_query, all_context, all_responses):\n",
    "    print(\"Generating response for query:\", query)\n",
    "    # generate context for new query\n",
    "    context, files = retrieve_relevant_chunks(collection_name, query)\n",
    "    \n",
    "    # unique_files = np.unique(files)\n",
    "    #file_rank = file_ratios(files)\n",
    "    \n",
    "\n",
    "    system_role = \"You are a specialized assistant that only provides advice on dog-related veterinary care. If a user asks about any other animal or topic outside of dog health, politely decline to answer and remind them that you only provide information about dogs. You will always start by asking the user their dog's name, age, and breed if they didn't already provide it.\"\n",
    "    # Combine retrieved chunks into a single string\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "    # append query and context to the running lists\n",
    "    all_query.append(query)\n",
    "    all_context.append(context_text)\n",
    "\n",
    "    # create the messages object using all the queries and contexts\n",
    "    messages = [{\"role\": \"system\", \"content\": system_role}]\n",
    "\n",
    "    for i in range(len(all_query)):\n",
    "        messages.append({\"role\": \"system\", \"content\": \"Use this context to answer my following question: \" + all_context[i]})\n",
    "        messages.append({\"role\": \"user\", \"content\": all_query[i]})\n",
    "        if i < len(all_responses):\n",
    "            messages.append({\"role\": \"system\", \"content\": all_responses[i]})\n",
    "    \n",
    "    # print(messages)\n",
    "\n",
    "\n",
    "    # Generate a response using GPT-4\n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=messages\n",
    "    )\n",
    "    all_responses.append(completion.choices[0].message.content)\n",
    "    return all_query, all_context, all_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground (use this to test querys in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# collection_name = \"LLM_V1\"\n",
    "# query = \"After we walk, my dog is always itchy\"\n",
    "# response, file_rank = generate_response(collection_name, query)\n",
    "# print_markdown(response.content)\n",
    "\n",
    "\n",
    "# print(\"files used: \\n\")\n",
    "# for file in file_rank:\n",
    "#     print(f\"{file}, {file_rank[file]} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight Flask Server (for Frontend API testing) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.17.96.147:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [09/Dec/2024 11:14:09] \"OPTIONS /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response for query: My dog cant sleep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Dec/2024 11:14:11] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 11:14:50] \"OPTIONS /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response for query: Sure! My dog's name is Mox. He is 11 years old and is a wheaten terrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Dec/2024 11:15:48] \"OPTIONS /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response for query: My dog is having trouble sleeping\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Dec/2024 11:15:50] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 11:16:05] \"OPTIONS /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response for query: My dogs name is Mox he is 11 years old and is a pug\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Dec/2024 11:16:12] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 11:16:30] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 11:16:56] \"OPTIONS /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response for query: What is an orthopedic bed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Dec/2024 11:17:00] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2024 11:17:32] \"OPTIONS /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response for query: Thanks so much\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Dec/2024 11:17:34] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "collection_name = \"LLM_V1\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # This will enable CORS for all routes\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def query_llm():\n",
    "    data = request.json\n",
    "    new_query = data.get('new_query')\n",
    "    queries = data.get('queries')\n",
    "    contexts = data.get('contexts')\n",
    "    responses = data.get('responses')\n",
    "    \n",
    "    # if not new_query or not queries or not contexts or not responses:\n",
    "    #     return jsonify({'error': 'New query, queries, contexts, and responses must be provided'}), 400\n",
    "\n",
    "    try:\n",
    "        updated_queries, updated_contexts, updated_responses = generate_response(collection_name, new_query, queries, contexts, responses)\n",
    "        return jsonify({\n",
    "            'queries': updated_queries,\n",
    "            'contexts': updated_contexts,\n",
    "            'responses': updated_responses\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
