{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pronova LLM Run Model #\n",
    "## Use this notebook to do the following ##\n",
    "- Run the current model on a query\n",
    "- start a flask api server that accepts a query and will return a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load require libraries\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Qdrant connection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Qdrant API key from the environment variable\n",
    "Qdrant_api_key = os.getenv('Qdrant_API_KEY')\n",
    "if not Qdrant_api_key:\n",
    "    raise ValueError(\"No Qdrant API key found in environment variables\")\n",
    "Qdrant_url = os.getenv('Qdrant_URL')\n",
    "if not Qdrant_url:\n",
    "    raise ValueError(\"No Qdrant URL found in environment variables\")\n",
    "\n",
    "\n",
    "# Initialize Qdrant client\n",
    "try:\n",
    "    Qclient = QdrantClient(\n",
    "        url= Qdrant_url,\n",
    "        api_key=Qdrant_api_key\n",
    "    )\n",
    "    print(\"Successfully connected to Qdrant\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Qdrant: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup OpenAI connection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OpenAI API key from the environment variable\n",
    "OpenAI_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not OpenAI_api_key:\n",
    "    raise ValueError(\"No OpenAI API key found in environment variables\")\n",
    "\n",
    "OpenAI.api_key = OpenAI_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an OpenAI embedding from a text segment (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the embedding of a text\n",
    "def get_embedding(text):\n",
    "    client = OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve similar chunks from query (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(collection_name, query, top_k=10):\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    search_result = Qclient.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    contexts = [result.payload[\"text\"] for result in search_result]\n",
    "    files = [result.payload.get(\"source_file\") for result in search_result]\n",
    "    \n",
    "    return contexts, files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank response source importance (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def file_ratios(files):\n",
    "    total_files = len(files)\n",
    "    counts = Counter(files)\n",
    "    return {file: count*100 / total_files for file, count in counts.items()}\n",
    "\n",
    "\n",
    "# file_ratios([\"a\", \"a\", \"b\", \"c\"])\n",
    "# {'a': 0.5, 'b': 0.25, 'c': 0.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Print Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_markdown(md_text):\n",
    "    display(Markdown(md_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Response from Query (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_response(collection_name, query, all_query, all_context, all_responses):\n",
    "    print(\"Generating response for query:\", query)\n",
    "    # generate context for new query\n",
    "    context, files = retrieve_relevant_chunks(collection_name, query)\n",
    "    \n",
    "    # unique_files = np.unique(files)\n",
    "    #file_rank = file_ratios(files)\n",
    "    \n",
    "\n",
    "    system_role = \"You are a specialized assistant that only provides advice on dog-related veterinary care. If a user asks about any other animal or topic outside of dog health, politely decline to answer and remind them that you only provide information about dogs. You will always start by asking the user their dog's name, age, and breed if they didn't already provide it.\"\n",
    "    # Combine retrieved chunks into a single string\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "    # append query and context to the running lists\n",
    "    all_query.append(query)\n",
    "    all_context.append(context_text)\n",
    "\n",
    "    # create the messages object using all the queries and contexts\n",
    "    messages = [{\"role\": \"system\", \"content\": system_role}]\n",
    "\n",
    "    for i in range(len(all_query)):\n",
    "        messages.append({\"role\": \"system\", \"content\": \"Use this context to answer my following question: \" + all_context[i]})\n",
    "        messages.append({\"role\": \"user\", \"content\": all_query[i]})\n",
    "        if i < len(all_responses):\n",
    "            messages.append({\"role\": \"system\", \"content\": all_responses[i]})\n",
    "    \n",
    "    # print(messages)\n",
    "\n",
    "\n",
    "    # Generate a response using GPT-4\n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=messages\n",
    "    )\n",
    "    all_responses.append(completion.choices[0].message.content)\n",
    "    return all_query, all_context, all_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground (use this to test querys in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection_name = \"LLM_PetMD\"\n",
    "# query = \"is this working?\"\n",
    "# all_query = []\n",
    "# all_context = []\n",
    "# all_responses = []\n",
    "\n",
    "# aq, ac, ar = generate_response(collection_name, query, all_query, all_context, all_responses)\n",
    "\n",
    "# print(ar[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight Flask Server (for Frontend API testing) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "collection_name = \"LLM_PetMD\" # for eyals db\n",
    "# collection_name = \"LLM_V1\"  # for other db\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # This will enable CORS for all routes\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def query_llm():\n",
    "    data = request.json\n",
    "    new_query = data.get('new_query')\n",
    "    queries = data.get('queries')\n",
    "    contexts = data.get('contexts')\n",
    "    responses = data.get('responses')\n",
    "    \n",
    "    # if not new_query or not queries or not contexts or not responses:\n",
    "    #     return jsonify({'error': 'New query, queries, contexts, and responses must be provided'}), 400\n",
    "\n",
    "    try:\n",
    "        updated_queries, updated_contexts, updated_responses = generate_response(collection_name, new_query, queries, contexts, responses)\n",
    "        return jsonify({\n",
    "            'queries': updated_queries,\n",
    "            'contexts': updated_contexts,\n",
    "            'responses': updated_responses\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
