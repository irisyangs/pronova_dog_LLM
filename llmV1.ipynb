{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pronova LLM V1 **(Outdated now. Use new \"Run\" and \"Train\" notebooks)** #\n",
    "## Use this notebook to do the following ##\n",
    "- Create Qdrant collections\n",
    "- Chunk text files into smaller chunks\n",
    "- Create embeddings for data chunks and querys\n",
    "- Delete qdrant vectors\n",
    "- Run the current model on a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load require librarys\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Qdrant connection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Qdrant\n"
     ]
    }
   ],
   "source": [
    "# Get the Qdrant API key from the environment variable\n",
    "Qdrant_api_key = os.getenv('Qdrant_API_KEY')\n",
    "if not Qdrant_api_key:\n",
    "    raise ValueError(\"No Qdrant API key found in environment variables\")\n",
    "Qdrant_url = os.getenv('Qdrant_URL')\n",
    "if not Qdrant_url:\n",
    "    raise ValueError(\"No Qdrant URL found in environment variables\")\n",
    "\n",
    "\n",
    "# Initialize Qdrant client\n",
    "try:\n",
    "    Qclient = QdrantClient(\n",
    "        url= Qdrant_url,\n",
    "        api_key=Qdrant_api_key\n",
    "    )\n",
    "    print(\"Successfully connected to Qdrant\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Qdrant: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup OpenAI connection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OpenAI API key from the environment variable\n",
    "OpenAI_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not OpenAI_api_key:\n",
    "    raise ValueError(\"No OpenAI API key found in environment variables\")\n",
    "\n",
    "OpenAI.api_key = OpenAI_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Qdrant Collection (function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create collection\n",
    "def create_qdrant_collection(collection_name):\n",
    "    try:\n",
    "        Qclient.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),\n",
    "        )\n",
    "        print(f\"Collection '{collection_name}' created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create collection '{collection_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an OpenAI embedding from a text segment (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the embedding of a text\n",
    "def get_embedding(text):\n",
    "    client = OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn a file into chunks (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a text file and chunk its content\n",
    "def chunk_text_from_file(file_path, chunk_size=400):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embedding from a list of chunks (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embeddings for a list of text chunks\n",
    "def get_embeddings_for_chunks(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embedding = get_embedding(chunk)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get largest Qdrant ID (function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qdrant_next_id(collection_name):\n",
    "    try:\n",
    "        collection = Qclient.get_collection(collection_name=collection_name)\n",
    "        id = collection.points_count\n",
    "        if id == None:\n",
    "            # print(\"id was zero\")\n",
    "            return 0\n",
    "        else:\n",
    "            # print(f\"Next ID in collection '{collection_name}' is {id}\")\n",
    "            return id\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get next ID from collection '{collection_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert lists of documents to Qdrant (Function) ###\n",
    "#### Mind the parameters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upsert embeddings into Qdrant\n",
    "# def upsert_embeddings(collection_name, embeddings, chunks):\n",
    "def upsert_embeddings(collection_name, embeddings, chunks, filename):\n",
    "    largest_id = get_qdrant_next_id(collection_name)\n",
    "    points = []\n",
    "    for i in range(len(embeddings)):\n",
    "        points.append(\n",
    "            {\n",
    "                \"id\": i + largest_id + 1,\n",
    "                \"vector\": embeddings[i],\n",
    "                \"payload\": {\n",
    "                    \"text\": chunks[i],   # Attach the chunk as payload\n",
    "                    \"source_file\": filename  # Add source file\n",
    "                }            \n",
    "            }\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        Qclient.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points\n",
    "        )\n",
    "        print(\"Embeddings upserted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upsert embeddings: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve similar chunks from query (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(collection_name, query, top_k=10):\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    search_result = Qclient.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    contexts = [result.payload[\"text\"] for result in search_result]\n",
    "    files = [result.payload.get(\"source_file\") for result in search_result]\n",
    "    \n",
    "    return contexts, files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Response from Query (Function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def file_ratios(files):\n",
    "    total_files = len(files)\n",
    "    counts = Counter(files)\n",
    "    return {file: count*100 / total_files for file, count in counts.items()}\n",
    "\n",
    "\n",
    "# file_ratios([\"a\", \"a\", \"b\", \"c\"])\n",
    "# {'a': 0.5, 'b': 0.25, 'c': 0.25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_response(collection_name, query):\n",
    "    context, files = retrieve_relevant_chunks(collection_name, query)\n",
    "    \n",
    "    # unique_files = np.unique(files)\n",
    "    file_rank = file_ratios(files)\n",
    "    \n",
    "\n",
    "    system_role = \"You are a specialized assistant that only provides advice on dog-related veterinary care. If a user asks about any other animal or topic outside of dog health, politely decline to answer and remind them that you only provide information about dogs.\"\n",
    "\n",
    "    # If you don't know the answer to a question, let the user know that you're not sure and suggest that they consult a veterinarian for more information.\n",
    "\n",
    "    # Combine retrieved chunks into a single string\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "\n",
    "    # Generate a response using GPT-4\n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_role},\n",
    "            {\"role\": \"user\", \"content\": \"context\" + context_text},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message, file_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete all entries in a collection (Function) ###\n",
    "*BE CAREFUL WITH THIS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model(collection_name):\n",
    "    confirmation = input(\"Type 'DELETE' to confirm deletion of entries in \" + collection_name + \". This cannot be undone. Type anything else to abort\")\n",
    "    if confirmation == \"DELETE\" :\n",
    "        try:\n",
    "            Qclient.delete_collection(collection_name=collection_name)\n",
    "            print(f\"Collection '{collection_name}' deleted successfully\")\n",
    "            create_qdrant_collection(collection_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete collection '{collection_name}': {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(\"Deletion aborted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qclient.get_collections() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Print Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_markdown(md_text):\n",
    "    display(Markdown(md_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Files From Folder (function) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_from_folder(collection_name, file_path, limit):\n",
    "    \n",
    "    #number of files in the folder \n",
    "    num_files = len([name for name in os.listdir(file_path) if os.path.isfile(os.path.join(file_path, name))])\n",
    "    if limit == None:\n",
    "        limit = num_files\n",
    "    current_file = 1\n",
    "\n",
    "    for filename in os.listdir(file_path):\n",
    "        if current_file > limit:\n",
    "            break\n",
    "        curr_file_path = os.path.join(file_path, filename)\n",
    "        print(f\"Processing file {current_file} of max {limit}: {filename}\")\n",
    "        \n",
    "        # # Chunk the file\n",
    "        chunks = chunk_text_from_file(curr_file_path)\n",
    "        # Get embeddings for the chunks\n",
    "        embeddings = get_embeddings_for_chunks(chunks)\n",
    "        # Upsert the embeddings into Qdrant\n",
    "        upsert_embeddings(collection_name, embeddings, chunks, filename)\n",
    "        current_file += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground ##\n",
    "Use this section to test functions and play around with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete_collection(\"LLM_V1\")\n",
    "filepath = \"scrapingDemo/ScrapedFiles_petMD_behavior\"\n",
    "collection_name = \"LLM_V2\"\n",
    "limit = None\n",
    "# delete_collection(collection_name)\n",
    "process_files_from_folder(collection_name, filepath, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"LLM_V1\"\n",
    "query = \"After we walk, my dog is always itchy\"\n",
    "response, file_rank = generate_response(collection_name, query)\n",
    "print_markdown(response.content)\n",
    "\n",
    "\n",
    "print(\"files used: \\n\")\n",
    "for file in file_rank:\n",
    "    print(f\"{file}, {file_rank[file]} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.17.96.147:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Dec/2024 10:29:04] \"OPTIONS /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:29:10] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:30:03] \"OPTIONS /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:30:08] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:31:00] \"OPTIONS /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:31:22] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:32:38] \"OPTIONS /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:32:43] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:33:27] \"OPTIONS /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:33:29] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:33:39] \"OPTIONS /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:33:40] \"POST /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:34:05] \"OPTIONS /query HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Dec/2024 10:34:09] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "collection_name = \"LLM_V1\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # This will enable CORS for all routes\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def query_llm():\n",
    "    data = request.json\n",
    "    query = data.get('query')\n",
    "    if not query:\n",
    "        return jsonify({'error': 'No query provided'}), 400\n",
    "\n",
    "    try:\n",
    "        response, files = generate_response(collection_name, query)\n",
    "        return jsonify({\n",
    "            'response': response.content\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
